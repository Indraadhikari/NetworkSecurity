# Website Crawler

I created a Website Crawler application for this project that can retrieve the data and information from the website we provided as an input. The program will start by searching the root webpage, then go on to other links inside that page, then into those links, and so on until the crawler's depth is reached. **Python** programming is being used to create this project. **Request**, **BeautifulSoup**, **pandas**, **Tkinter**, **concurrent.features**, and other libraries are primarily utilized in this project. In order to increase the program speed, I'm utilizing the concurrent.features package and the **multiprocessing** approach. Additionally, the Tkinter library is being used to construct the user interface, where we can enter program inputs like the root URL and the depth for the crawler, which are crucial inputs for a web crawler. By submitting regular expressions to the computer program, the user may search their sensitive data using this project's regular expression input box. Also, when the application has run, a **CSV file** containing the email addresses and phone numbers that appear on the site pages will be retrieved. Last but not the least, we can get the top five most commonly used words in the web site. Overall, this project's website crawler is able to retrieve all of the web pages' comment sections as well as certain user-delivered data using the specified root web address and regular expression.
![image](https://github.com/user-attachments/assets/68a8c78a-c2c5-4d53-afb1-35e1fcfcbeeb)
