# Website Crawler

 A computer software used to visit and collect the necessary data from a specific website is referred to as a "website crawler." The crawler downloads the webpages automatically in the basis of root website URL. In the other word, the website crawler is the software bot to operate and process internet data automatically. In order to be able to access the information when required, such a bot must understand what every website on the internet is about. I created a Website Crawler application for this project that can retrieve the data and information from the website we provided as an input. The program will start by searching the root webpage, then go on to other links inside that page, then into those links, and so on until the crawler's depth is reached. Python programming is being used to create this project. Request, BeautifulSoup, pandas, Tkinter, concurrent.features, and other libraries are primarily utilized in this project. In order to increase the program speed, I'm utilizing the concurrent.features package and the multiprocessing approach. Additionally, the Tkinter library is being used to construct the user interface, where we can enter program inputs like the root URL and the depth for the crawler, which are crucial inputs for a web crawler. By submitting regular expressions to the computer program, the user may search their sensitive data using this project's regular expression input box. Also, when the application has run, a CSV file containing the email addresses and phone numbers that appear on the site pages will be retrieved. Last but not the least, we can get the top five most commonly used words in the web site. Overall, this project's website crawler is able to retrieve all of the web pages' comment sections as well as certain user-delivered data using the specified root web address and regular expression.
